{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Линейная функция"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fc29a82ba2e4e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Линейная функция имеет вид:\n",
    "\n",
    "y = w1x1 + ... + wDxD + w0\n",
    "\n",
    "где \n",
    "y - целевая переменная (таргет),\n",
    "(x1, ... , xD) - вектор, соответствующий объекту выборки (вектор признаков) \n",
    "(w1, ... , wD) - параметры модели (вектор весов)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7f838c92d5e654"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Линейная регрессия и метод наименьших квадратов (МНК)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5e53ca0fbcdffed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "__МНК: точный аналитический метод__\n",
    "\n",
    "Пусть у нас задан датасет (X, y) где \n",
    "y - вектор значений целевой переменной\n",
    "X - матрица объекты-признаки\n",
    "\n",
    "Мы хотим моделировать зависимость yi от xi как линейную функцию со свободным членом. Общий вид такой функции выглядит следующим образом:\n",
    "\n",
    "fw(xi) = (wixi) + w0\n",
    "\n",
    "Задача: научиться измерять качество модели и минимизировать ее ошибку, изменяя обучаемые параметры.\n",
    "\n",
    "Обучаемые параметры - это веса w.\n",
    "\n",
    "Функция, которая оценивает как часто ошибается модель, называется функцией потерь или лоссом (loss function).\n",
    "\n",
    "В качестве лосса возьмем среднеквадратическое отклонение, MSE (Mean Squared Error):\n",
    "\n",
    "MSE(f, X, y) = 1/NΣ|y-Xw|^2\n",
    "\n",
    "__Основные принципы линейной регрессии:__\n",
    "\n",
    "1) Матрица признаков (X): Представляет собой матрицу, где каждый столбец (x1, ..., xD) представляет собой признак, а каждая строка соответствует отдельному наблюдению.\n",
    "    __                         __\n",
    "    | (x1,1) (x1,2) ... (x1,D)  |\n",
    "X = | (x2,1) (x2,2) ... (x2,D)  |\n",
    "    |  ...     ...  ...   ...   |\n",
    "    | (xN,1) (xN,2) ... (xN,D)  |\n",
    "    --                         --\n",
    "    \n",
    "где\n",
    "N - номер отдельного наблюдения (временной ряд)\n",
    "D - номер признака (кол-во признаков)\n",
    "\n",
    "2) Весовой вектор (w): Представляет собой вектор весов, где каждый вес w_i соответствует весу, присвоенному соответствующему признаку x_i.\n",
    "    __    __\n",
    "    | (w1) |\n",
    "W = | (w2) |\n",
    "    |  ... |\n",
    "    | (wD) |\n",
    "    --    --\n",
    "\n",
    "3) Линейная комбинация (Xw): Это выражение представляет собой линейную комбинацию столбцов матрицы X с использованием весового вектора w.\n",
    "\n",
    "Xw = w1x1 + w2x2 + ... + wDxD\n",
    "\n",
    "4) Задача регрессии: Задача заключается в нахождении вектора весов w таким образом, чтобы линейная комбинация Xw наилучшим образом \n",
    "приближала столбец y (вектор ответов) в смысле евклидовой нормы (или среднеквадратичной ошибки).\n",
    "\n",
    "minimize || Xw - y ||^2\n",
    "\n",
    "где || Xw - y ||^2 представляет собой квадрат евклидовой нормы разности между предсказанным значением Xw и реальным значением y.\n",
    "\n",
    "5) Общий случай линейной регрессии с D признаками означает работу в D+1 - мерном пространстве, где D - кол-во признаков, а +1 - целевая переменная.\n",
    "\n",
    "То есть, если у нас имеется 3 признака (x1, x2, x3) и 1 целевая переменная (y), то каждое наблюдение можно представить в четырехмерном пространстве. \n",
    "Каждая точка будет иметь координаты (x1, x2, x3, y), и наша задача состоит в том, чтобы найти гиперплоскость, которая лучше всего аппроксимирует распределение \n",
    "точек в этом __четырехмерном пространстве__.\n",
    "\n",
    "__Задачу регрессии можно сформулировать следующим образом__: найти линейную комбинациб столбцов x1, ... , xD, которая наилучшим способом приближает столбец y по\n",
    "евклидовой норме - то есть проекцию вектора y на подпространство, образованное векторами x1, ... , xD. Выразим это в матричном виде:\n",
    "\n",
    "Xt(y - Xw) = 0\n",
    "\n",
    "где Xt - транспонированная матрица, \n",
    "\n",
    "из этого уравнения легко вычислить w: \n",
    "\n",
    "__w = (XT*X)^(-1)*XT*y__\n",
    "\n",
    "Полезно знать! ->\n",
    "\n",
    "Для вычисления w нам приходится обращать (квадратную) матрицу XtX, что возможно, только если она невырожденна.\n",
    "\n",
    "Матрица XtX невырожденна тогда и только тогда, когда её ранг равен числу её столбцов, что равно числу столбцов матрицы X.\n",
    "Иными словами, формула регрессии поломается, только если столбцы матрицы линейно зависимы. Столбцы матрицы X – это признаки.\n",
    "А если наши признаки линейно зависимы, то, наверное, что-то идёт не так и мы должны выкинуть часть из них, чтобы остались только линейно независимые.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14edbcb9806cb619"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = [1, 2, 3]\n",
    "for i in dataset:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T18:42:42.080220900Z",
     "start_time": "2023-12-01T18:42:42.068252800Z"
    }
   },
   "id": "94e453a7c862e480"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T18:42:32.231004700Z",
     "start_time": "2023-12-01T18:42:32.216535100Z"
    }
   },
   "id": "3a63ed6b75314755"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
